{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\"/scratch/tjf324/pytorch-pretrained-BERT/\")\n",
    "from pytorch_pretrained_bert import modeling, tokenization\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "import logging\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "from language_modeling.runners import (\n",
    "    tokenize_example, InputExample,\n",
    "    convert_example_to_features, features_to_data,\n",
    ")\n",
    "\n",
    "WNLI_TRAIN_PATH = \"/scratch/tjf324/data/glue_auto_dl/WNLI/train.tsv\"\n",
    "WNLI_DEV_PATH = \"/scratch/tjf324/data/glue_auto_dl/WNLI/dev.tsv\"\n",
    "WNLI_TEST_PATH = \"/scratch/tjf324/data/glue_auto_dl/WNLI/test.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "def get_pos(sent):\n",
    "    return [token.pos_ for token in nlp(sent)]\n",
    "\n",
    "def is_noun(pos):\n",
    "    return pos in [\"PRON\", \"PROPN\", \"NOUN\"]\n",
    "\n",
    "def get_pos_dict(sent):\n",
    "    return {\n",
    "        token.text.lower(): token.pos_\n",
    "        for token in nlp(sent)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "bert_model_name = \"bert-large-uncased\"\n",
    "max_sequence_length = 128\n",
    "MASK = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modeling.BertForPreTraining.from_pretrained(bert_model_name)\n",
    "model.to(device);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(WNLI_TRAIN_PATH, sep=\"\\t\")\n",
    "val_df = pd.read_csv(WNLI_DEV_PATH, sep=\"\\t\")\n",
    "\n",
    "test_df = pd.read_csv(WNLI_TEST_PATH, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5086614173228347\n",
      "0.5633802816901409\n"
     ]
    }
   ],
   "source": [
    "print((train_df[\"label\"]==0).mean())\n",
    "print((val_df[\"label\"]==0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_dict(pos_dict):\n",
    "    BLACKLIST = [\"he\", \"she\", \"it\", \"they\", \"who\", \"her\", \"we\", \"them\",\n",
    "                 \"him\", \"his\", 'their', \"hers\", \"his\", \"theirs\", \"i\", \"me\", \"you\",\n",
    "                 \"us\", ]\n",
    "    return {noun: pos \n",
    "            for noun, pos in pos_dict.items()\n",
    "            if is_noun(pos) and noun not in BLACKLIST}\n",
    "\n",
    "def get_token_groups(words):\n",
    "    return [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w)) for w in words]\n",
    "\n",
    "def get_pos_dict_and_token_groups(text):\n",
    "    pos_dict = filter_pos_dict(get_pos_dict(text))\n",
    "    tok_groups = get_token_groups(pos_dict.keys())\n",
    "    return pos_dict, tok_groups\n",
    "\n",
    "def get_length_info(tok_a, tok_b):\n",
    "    sent_a_lengths = set(len(a_i) for a_i in tok_a)\n",
    "    sent_b_lengths = set(len(b_i) for b_i in tok_b)\n",
    "    sent_a_one_length = len(sent_a_lengths) == 1\n",
    "    sent_b_one_length = len(sent_b_lengths) == 1\n",
    "    same_length = sent_a_lengths == sent_b_lengths\n",
    "    return sent_a_one_length, sent_b_one_length, same_length\n",
    "\n",
    "def mask_predict_one_example(tokenizer, model, row):\n",
    "    alt_ls = []\n",
    "    pred_result = True\n",
    "    example = InputExample(guid=0, text_a=row[\"sentence1\"], text_b=row[\"sentence2\"], is_next=True)\n",
    "\n",
    "    tokens_a_pos_dict, tokens_a_token_groups = get_pos_dict_and_token_groups(example.text_a)\n",
    "    tokens_b_pos_dict, tokens_b_token_groups = get_pos_dict_and_token_groups(example.text_b)\n",
    "\n",
    "    tokenized_example = tokenize_example(example, tokenizer)\n",
    "\n",
    "    b_ids = tokenizer.convert_tokens_to_ids(tokenized_example.tokens_b)\n",
    "\n",
    "    sent_a_one_length, sent_b_one_length, same_length = get_length_info(tokens_a_token_groups, \n",
    "                                                                        tokens_b_token_groups)\n",
    "#         if sent_a_one_length and sent_b_one_length and same_length:\n",
    "#             pred_ls.append(False)\n",
    "#             all_alt_ls.append(alt_ls)\n",
    "#             continue \n",
    "\n",
    "    for i, tok_id in enumerate(b_ids):\n",
    "        if not any(tok_id == tok_group[0] for tok_group in tokens_b_token_groups):\n",
    "            continue\n",
    "        else:\n",
    "            to_mask = max(len(tok_group) \n",
    "                          for tok_group in tokens_b_token_groups \n",
    "                          if tok_id == tok_group[0])\n",
    "\n",
    "        tokenized_example_changed = copy.deepcopy(tokenized_example)\n",
    "        for idx in range(i, i+to_mask):\n",
    "            tokenized_example_changed.tokens_b[idx] = MASK\n",
    "\n",
    "        features = convert_example_to_features(tokenized_example_changed, tokenizer, \n",
    "                                               max_sequence_length, select_prob=0.0)\n",
    "\n",
    "        batch = features_to_data([features]).to(device)\n",
    "        with torch.no_grad():\n",
    "            result = model(batch.input_ids, batch.segment_ids, batch.input_mask)\n",
    "\n",
    "        masked_indices = np.arange(batch.input_ids.shape[1])[batch.input_ids[0].cpu().numpy() == 103]\n",
    "        first_masked_index = masked_indices[0]\n",
    "\n",
    "\n",
    "        preds_first_token = result[0][0, first_masked_index, :].cpu().numpy()\n",
    "        possible_first_tokens = [tok_id] \n",
    "        # Possible first tokens for b:\n",
    "        # Tokens in a that are not in b once masked (no repeats)\n",
    "        # Though this does not seem to be beneficial so disabling for now\n",
    "        # We're already overpredicting True...\n",
    "        remaining_b_tok_ids = tokenizer.convert_tokens_to_ids(tokenized_example_changed.tokens_b)\n",
    "        possible_first_tokens += [tok_id_a[0] \n",
    "                                  for tok_id_a, tok_a in zip(tokens_a_token_groups, tokens_a_pos_dict)\n",
    "                                  if tok_a not in tokenized_example_changed.tokens_b \n",
    "                                  and tok_a[0] != tok_id]   # This handles weird cases with plurals...\n",
    "\n",
    "\n",
    "        kept_preds_first_token = preds_first_token[possible_first_tokens]\n",
    "\n",
    "\n",
    "\n",
    "        most_predicted = np.argmax(kept_preds_first_token)\n",
    "        # If different than 0, than we're predicting something else....\n",
    "        actually_predicted = tokenizer.ids_to_tokens[possible_first_tokens[most_predicted]]\n",
    "        replaced = tokenizer.ids_to_tokens[tok_id]\n",
    "        alt_ls.append(example.text_b.lower().replace(replaced, actually_predicted.upper()))\n",
    "\n",
    "        if most_predicted:\n",
    "            pred_result = False\n",
    "    return pred_result, alt_ls, tokens_a_pos_dict, tokens_a_token_groups, tokens_b_pos_dict, tokens_b_token_groups\n",
    "\n",
    "    \n",
    "\n",
    "def masking_predictor(df, tokenizer, model):\n",
    "    all_pos_dicts_sent1 = []\n",
    "    all_pos_dicts_sent2 = []\n",
    "    all_tok_groups_sent1 = []\n",
    "    all_tok_groups_sent2 = []\n",
    "    pred_ls = []\n",
    "    all_alt_ls = []\n",
    "    for _, row in tqdm.tqdm_notebook(df.iterrows(), total=len(df)):\n",
    "        pred, alt_ls, pos_dict_a, tok_group_a, pos_dict_b, tok_group_b = mask_predict_one_example(tokenizer, \n",
    "                                                                                                  model, row)\n",
    "        all_pos_dicts_sent1.append(pos_dict_a)\n",
    "        all_tok_groups_sent1.append(tok_group_a)\n",
    "        all_pos_dicts_sent2.append(pos_dict_b)\n",
    "        all_tok_groups_sent2.append(tok_group_b)\n",
    "        pred_ls.append(pred)\n",
    "        all_alt_ls.append(alt_ls)\n",
    "        \n",
    "    pred_arr = np.array(pred_ls)\n",
    "    return pred_arr, all_alt_ls, all_pos_dicts_sent1, all_pos_dicts_sent2, all_tok_groups_sent1, all_tok_groups_sent2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa12837dfb5e49bfbdd826d4ff8bc75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=635), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train acc:  0.5748031496062992\n",
      "mean train pred: 0.6047244094488189\n"
     ]
    }
   ],
   "source": [
    "train_pred_arr, train_all_alt_ls, pos_a, pos_b, tok_a, tok_b = masking_predictor(train_df, tokenizer, model)\n",
    "\n",
    "# val_pred_arr, val_all_alt_ls, *_ = masking_predictor(val_df, tokenizer, model)\n",
    "\n",
    "print(\"Train acc: \", (train_pred_arr==train_df[\"label\"]).mean())\n",
    "# print(\"Val acc: \", (val_pred_arr==val_df[\"label\"]).mean())\n",
    "\n",
    "print(\"mean train pred:\", train_pred_arr.mean())\n",
    "# print(\"mean val pred:\", val_pred_arr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKLIST = [\"he\", \"she\", \"it\", \"they\", \"who\", \"her\", \"we\", \"them\",\n",
    "             \"him\", \"his\", 'their', \"hers\", \"his\", \"theirs\", \"i\", \"me\", \"you\",\n",
    "             \"us\"]\n",
    "\n",
    "def get_noun_chunks(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    nc = list(set(filter(lambda n: n.text.lower() not in BLACKLIST, doc.noun_chunks)))\n",
    "    return nc\n",
    "\n",
    "def get_masked_examples(sent1, begin, end, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate the masked examples\n",
    "    \"\"\"\n",
    "    base_example = InputExample(guid=0, text_a=sent1, text_b=begin + end, is_next=True)\n",
    "    tokenized_example = tokenize_example(base_example, tokenizer)\n",
    "    \n",
    "    begin_tok = tokenizer.tokenize(begin)\n",
    "    end_tok = tokenizer.tokenize(end)\n",
    "    print(end_tok)\n",
    "    masked_examples = []\n",
    "    right = tokenizer.convert_tokens_to_ids(end_tok)\n",
    "    for i in range(len(end_tok)):\n",
    "        masked_end_tok = end_tok.copy()\n",
    "        masked_end_tok[i] = MASK\n",
    "        tokenized_example_changed = copy.deepcopy(tokenized_example)\n",
    "        tokenized_example_changed.tokens_a = tokenized_example_changed.tokens_a + begin_tok + masked_end_tok \n",
    "        tokenized_example_changed.tokens_b = [\"glue\"]\n",
    "#         tokenized_example_changed.tokens_b = begin_tok + masked_end_tok \n",
    "        masked_examples.append(tokenized_example_changed)\n",
    "    return masked_examples, right\n",
    "    \n",
    "\n",
    "def get_mean_predictions(model, masked_examples, right):\n",
    "    \"\"\"\n",
    "    Once we have a sentence, see how likely the end is when removing words one by one\n",
    "    \"\"\"\n",
    "    features = [convert_example_to_features(ex, tokenizer, max_sequence_length, select_prob=0.0)\n",
    "                for ex in masked_examples]\n",
    "    batch = features_to_data(features).to(device)\n",
    "    with torch.no_grad():\n",
    "        result = model(batch.input_ids, batch.segment_ids, batch.input_mask)\n",
    "    ids = np.arange(batch.input_ids.shape[1])\n",
    "    probs = []\n",
    "    for i, right_idx in enumerate(right):\n",
    "        masked_idx = ids[batch.input_ids[i].cpu().numpy() == 103]\n",
    "        assert(len(masked_idx) == 1)\n",
    "        masked_idx = masked_idx[0]\n",
    "        pred_token = result[0][0, masked_idx, :].cpu().numpy()\n",
    "        prob = np.exp(pred_token[right_idx])/np.exp(pred_token).sum()\n",
    "        probs.append(prob)\n",
    "    probs = np.array(probs)\n",
    "    return probs.mean()\n",
    "\n",
    "def filling_predictor(df, tokenizer, model):\n",
    "    \"\"\"\n",
    "    The approach here is closer to `A simple method for commonsense reasoning`\n",
    "    e.g:\n",
    "        text_a = \"the yellow duck liked the fish because it was beautiful\"\n",
    "        text_b = \"the fish was beautiful\"\n",
    "    We produce:\n",
    "        text_b_alt_1 = mean(\"the fish was [...]\" -> beautiful ; \"the fish [...] beautiful\" -> was)\n",
    "        text_b_alt_2 = mean(\"the yellow duck was [...]\" -> beautiful ; \"the yellow duck [...] beautiful\" -> was)\n",
    "    If it agrees with initial sentence, then keep that one, otherwise discard. \n",
    "    The nice thing is we can operate at the noun chunk level\n",
    "    \"\"\"\n",
    "    text_a = \"the trophy didn't fit in the bag because it was too big. \"\n",
    "    masked_examples, right = get_masked_examples(text_a, \"the trophy\", \"was too big\", tokenizer)\n",
    "    print(get_mean_predictions(model, masked_examples, right))\n",
    "    masked_examples, right = get_masked_examples(text_a, \"the bag\", \"was too big\", tokenizer)\n",
    "    print(get_mean_predictions(model, masked_examples, right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'too', 'big']\n",
      "0.80300575\n",
      "['was', 'too', 'big']\n",
      "0.69981575\n"
     ]
    }
   ],
   "source": [
    "filling_predictor(None, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John couldn't see the stage with Billy in front of him because he is so short. John is so short. BILLY is so short. True False\n",
      "\n",
      "When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth. mother was careful not to disturb her, undressing and climbing back into her berth. mother was careful not to disturb her, undressing and climbing back into her BERTH. False True\n",
      "\n",
      "John was jogging through the park when he saw a man juggling watermelons. He was very impressive. John was very impressive. JOHN was very impressive. False True\n",
      "\n",
      "I took the water bottle out of the backpack so that it would be handy. I took the water bottle out of the backpack so that the backpack would be handy. i took the water bottle out of the BACKPACK so that the BACKPACK would be handy. False True\n",
      "\n",
      "The firemen arrived after the police because they were coming from so far away. The police were coming from so far away. the POLICE were coming from so far away. False True\n",
      "\n",
      "Lionel is holding captive a scientist, Dr. Vardi, who has invented a device that turns animals invisible; Lionel plans to use it on Geoffrey and send him to steal nuclear material from an army vault. Lionel plans to use it on Geoffrey and send Lionel to steal nuclear material from an army vault. lionel plans to use it on geoffrey and send lionel to steal nuclear material from an army VAULT. False True\n",
      "\n",
      "Sam pulled up a chair to the piano, but it was broken, so he had to stand instead. The piano was broken, so he had to stand instead. the PIANO was broken, so he had to stand instead. False True\n",
      "\n",
      "Paul tried to call George on the phone, but he wasn't successful. Paul wasn't successful. GEORGE wasn't successful. True False\n",
      "\n",
      "Look! There is a shark swimming right below that duck! It had better get away to safety fast! The shark had better get away to safety fast! the shark had better get away to SAFETY fast! False True\n",
      "\n",
      "The trophy doesn't fit into the brown suitcase because it is too large. The suitcase is too large. the SUITCASE is too large. False True\n",
      "\n",
      "Joe's uncle can still beat him at tennis, even though he is 30 years older. Joe is 30 years older. joe is 30 YEARS older. False True\n",
      "\n",
      "Grant worked hard to harvest his beans so he and his family would have enough to eat that winter, His friend Henry let him stack them in his barn where they would dry. Later, he and Tatyana would shell them and cook them for their Sunday dinners. His friend Henry let him stack Grant and his family in his barn where they would dry. his friend henry let him stack grant and his family in his BARN where they would dry. False True\n",
      "\n",
      "We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply too many of them. There were simply too many copies of the newsletter. there were simply too many copies of the NEWSLETTER. False True\n",
      "\n",
      "The table was piled high with food, and on the floor beside it there were crocks, baskets, and a five-quart pail of milk. Beside the table there were crocks, baskets, and a five-quart pail of milk. beside the table there were crocks, baskets, and a five-quart pail of MILK. True False\n",
      "\n",
      "Sam and Amy are passionately in love, but Amy's parents are unhappy about it, because they are snobs. Amy's parents are snobs. amy'LOVE parentLOVE are LOVEnobLOVE. True False\n",
      "\n",
      "John promised Bill to leave, so an hour later he left. Bill left. BILL left. False True\n",
      "\n",
      "Sara borrowed the book from the library because she needs it for an article she is working on. She reads it when she gets home from work. She reads the article when she gets home from work. she reads the article when she gets home from WORK. False True\n",
      "\n",
      "Fred is the only man alive who still remembers my father as an infant. When Fred first saw my father, he was twelve years old. When Fred first saw my father, My father was twelve years old. when fred first saw my father, my father was twelve YEARS old. False True\n",
      "\n",
      "Tatyana knew that Grandma always enjoyed serving an abundance of food to her guests. Now Tatyana watched as Grandma gathered Tatyana's small mother into a wide, scrawny embrace and then propelled her to the table, lifting her shawl from her shoulders, seating her in the place of honor, and saying simply: \"There's plenty.\" Grandma gathered Tatyana's small mother into a wide, scrawny embrace and then propelled Grandma to the table. grandma gathered tatyana's small mother into a wide, scrawny embrace and then propelled grandma to the TABLE. False True\n",
      "\n",
      "Madonna fired her trainer because she couldn't stand her boyfriend. She couldn't stand Madonna's boyfriend. she couldn't stand madonna's BOYFRIEND. False True\n",
      "\n",
      "I can't cut that tree down with that axe; it is too small. The axe is too small. the TREE is too small. True False\n",
      "\n",
      "I saw Jim yelling at some guy in a military uniform with a huge red beard. I don't know why he was, but he looked very unhappy. I don't know why the guy in uniform was, but he looked very unhappy. i don't know why the guy in UNIFORM was, but he looked very unhappy. False True\n",
      "\n",
      "Alice tried frantically to stop her daughter from chatting at the party, leaving us to wonder why she was behaving so strangely. Alice's daughter was behaving so strangely. alice's DAUGHTER was behaving so strangely. False True\n",
      "\n",
      "Sam pulled up a chair to the piano, but it was broken, so he had to stand instead. The chair was broken, so he had to stand instead. the PIANO was broken, so he had to stand instead. True False\n",
      "\n",
      "Mark was close to Mr. Singer's heels. He heard him calling for the captain, promising him, in the jargon everyone talked that night, that not one thing should be damaged on the ship except only the ammunition, but the captain and all his crew had best stay in the cabin until the work was over. Mr. Singer heard him calling for the captain mr. singer heard him calling for the CAPTAIN False True\n",
      "\n",
      "I couldn't put the pot on the shelf because it was too high. The shelf was too high. the POT was too high. True False\n",
      "\n",
      "John promised Bill to leave, so an hour later he left. John left. BILL left. True False\n",
      "\n",
      "Frank was upset with Tom because the toaster he had sold him didn't work. The toaster Frank had sold him didn't work. the toaster FRANK had sold him didn't work. False True\n",
      "\n",
      "Equally swoon-worthy is C.K. Dexter Haven, a pallid young dandy holding a jade-handled walking stick, with a poodle asleep at his feet. Equally swoon-worthy is C.K. Dexter Haven, a pallid young dandy holding a jade-handled walking stick, with a poodle asleep at Haven's feet. equally swoon-worthy is c.k. dexter haven, a pallid young dandy holding a jade-handled walking stick, with a poodle asleep at haven's FEET. True False\n",
      "\n",
      "Billy cried because Toby wouldn't accept his toy. Billy cried because Toby wouldn't accept Toby's toy. billy cried because toby wouldn't accept toby's TOY. False True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (pred, true, sent1, sent2, faked) in enumerate(zip(train_pred_arr, train_df[\"label\"], \n",
    "                                                   train_df['sentence1'], train_df['sentence2'], train_all_alt_ls)):\n",
    "    if not (true == pred):\n",
    "        print(sent1, sent2, faked[-1], bool(true), pred)\n",
    "        print()\n",
    "    if i > 80:    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trophy']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('trophy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not restricting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If not restricting / cancelling\n",
    "# print((train_pred_arr==train_df[\"label\"]).mean())\n",
    "# print((val_pred_arr==val_df[\"label\"]).mean())\n",
    "\n",
    "# print(\"mean train pred:\", train_pred_arr.mean())\n",
    "# print(\"mean val pred:\", val_pred_arr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred_v1(df, tokenizer, model):\n",
    "#     pred_ls = []\n",
    "#     all_alt_ls = []\n",
    "#     tokenized_examples = []\n",
    "#     for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "#         result_ls = [ ]\n",
    "#         alt_ls = []\n",
    "#         example = InputExample(\n",
    "#             guid=0,\n",
    "#             text_a=row[\"sentence1\"],\n",
    "#             text_b=row[\"sentence2\"],\n",
    "#             is_next=True,\n",
    "#         )\n",
    "#         tokenized_example = tokenize_example(example, tokenizer)\n",
    "#         tokenized_examples.append(tokenized_example)\n",
    "#         tokens_a_pos = get_pos(\" \".join(tokenized_example.tokens_a))\n",
    "#         tokens_b_pos = get_pos(\" \".join(tokenized_example.tokens_b))\n",
    "#         tokens_a_nouns = {\n",
    "#             tokenizer.vocab[word]\n",
    "#             for word, pos in zip(tokenized_example.tokens_a, tokens_a_pos)\n",
    "#             if is_noun(pos)\n",
    "#         }\n",
    "#         tokens_a_ids = np.array(list(tokens_a_nouns))\n",
    "#         pred_result = True\n",
    "#         for i in range(len(tokenized_example.tokens_b)):\n",
    "#             if not is_noun(tokens_b_pos[i]):\n",
    "#                 continue\n",
    "#             b_token = tokenized_example.tokens_b[i]\n",
    "#             tokenized_example = tokenize_example(example, tokenizer)\n",
    "#             tokenized_example.tokens_b[i] = MASK\n",
    "#             features = convert_example_to_features(tokenized_example, tokenizer, max_sequence_length, select_prob=0.0)\n",
    "#             batch = features_to_data([features]).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 result = model(\n",
    "#                     batch.input_ids, \n",
    "#                     batch.segment_ids, \n",
    "#                     batch.input_mask, \n",
    "#                 )\n",
    "#             masked_indices = np.arange(batch.input_ids.shape[1])[batch.input_ids[0].cpu().numpy()==103]\n",
    "#             assert len(masked_indices) == 1\n",
    "#             masked_index = masked_indices[0]\n",
    "#             srs = pd.Series(\n",
    "#                 result[0][0][masked_index].cpu().numpy()[tokens_a_ids],\n",
    "#                 index=[tokenizer.ids_to_tokens[i] for i in tokens_a_ids],\n",
    "#             ).sort_values()\n",
    "#             result_ls.append(srs.index[-1])\n",
    "#             if not srs.index[-1]==b_token:\n",
    "#                 pred_result = False\n",
    "#             alt_ls.append(\" \".join(tokenized_example.tokens_b).replace(\n",
    "#                 MASK, srs.index[-1].upper(),\n",
    "#             ))\n",
    "\n",
    "#         pred_ls.append(pred_result)\n",
    "#         all_alt_ls.append(alt_ls)\n",
    "#     pred_arr = np.array(pred_ls)\n",
    "#     return pred_arr, all_alt_ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
