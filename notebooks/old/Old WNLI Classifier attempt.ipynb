{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import randomVVdV\n",
    "import sys\n",
    "sys.path.append(\"/scratch/ratch/tjf324/pytorch-pretrained-BERT/\")\n",
    "from pytorch_pretrained_bert import modeling, tokenization\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from language_modeling.runners import (\n",
    "    tokenize_example, InputExample,\n",
    "    convert_example_to_features, features_to_data,\n",
    ")\n",
    "\n",
    "WNLI_TRAIN_PATH = \"/scratch/tjf324/data/glue_auto_dl/WNLI/train.tsv\"\n",
    "WNLI_DEV_PATH = \"/scratch/tjf324/data/glue_auto_dl/WNLI/dev.tsv\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_pos(sent):\n",
    "    return [token.pos_ for token in nlp(sent)]\n",
    "\n",
    "def is_noun(pos):\n",
    "    return pos in [\"PRON\", \"PROPN\", \"NOUN\"]\n",
    "\n",
    "def get_pos_dict(sent):\n",
    "    return {\n",
    "        token.text.lower(): token.pos_\n",
    "        for token in nlp(sent)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "bert_model_name = \"bert-large-uncased\"\n",
    "max_sequence_length = 128\n",
    "MASK = \"[MASK]\"\n",
    "\n",
    "model = modeling.BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = tokenization.BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True);\n",
    "\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=5e-5,\n",
    "                     warmup=0.1,\n",
    "                     t_total=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5086614173228347\n",
      "0.5633802816901409\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(WNLI_TRAIN_PATH, sep=\"\\t\")\n",
    "val_df = pd.read_csv(WNLI_DEV_PATH, sep=\"\\t\")\n",
    "\n",
    "print((train_df[\"label\"]==0).mean())\n",
    "print((val_df[\"label\"]==0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_in_blacklist(chunk):\n",
    "    BLACKLIST = [\"he\", \"she\", \"it\", \"they\", \"who\", \"her\", \"we\", \"them\",\n",
    "                 \"him\", \"his\", \"us\", \"me\", \"i\", \"anything\", \"anyone\", \"somebody\", \n",
    "                 \"someone\"]\n",
    "    return chunk.text.lower() not in BLACKLIST\n",
    "    \n",
    "\n",
    "def get_all_sentences(df, skip_false=False):\n",
    "    sent1s, sent2s = [], []\n",
    "    for _, row in tqdm.tqdm_notebook(df.iterrows(), total=len(df)):\n",
    "\n",
    "        if skip_false:\n",
    "            if not row.label:\n",
    "                continue\n",
    "        \n",
    "        sent1, sent2 = row.sentence1, row.sentence2\n",
    "\n",
    "        sent1s.append(sent1)\n",
    "        sent2s_this = {\"true\": sent2, \"alt\": []}\n",
    "\n",
    "        sent1_doc = nlp(sent1)\n",
    "        sent2_doc = nlp(sent2)\n",
    "        \n",
    "        \n",
    "        noun_chunks_sent1 = list(filter(remove_in_blacklist, sent1_doc.noun_chunks))\n",
    "        noun_chunks_sent2 = list(filter(remove_in_blacklist, sent2_doc.noun_chunks))\n",
    "\n",
    "        for chunk2 in noun_chunks_sent2[:1]:\n",
    "            for chunk1 in noun_chunks_sent1:\n",
    "          \n",
    "                if chunk2.root.text.lower() == chunk1.root.text.lower():\n",
    "                    continue\n",
    "                alt_sent = sent2.replace(chunk2.text, chunk1.text)\n",
    "                if alt_sent.lower() == sent2.lower():\n",
    "                    continue\n",
    "                sent2s_this[\"alt\"].append(alt_sent)\n",
    "        sent2s.append(sent2s_this)\n",
    "        \n",
    "    return sent1s, sent2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=635), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sent1s, sent2s = get_all_sentences(train_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=312), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "selected = []\n",
    "\n",
    "stop_at = 1000\n",
    "\n",
    "labels = train_df.label[:stop_at]\n",
    "labels = [1] * len(sent1s)\n",
    "\n",
    "model.train()\n",
    "\n",
    "steps = 0\n",
    "\n",
    "for sent1, sent2, label in tqdm.tqdm_notebook(zip(sent1s[:stop_at], \n",
    "                                                   sent2s[:stop_at], \n",
    "                                                   labels),\n",
    "                                               total=min(stop_at, len(sent1s))):\n",
    "    steps += 1\n",
    "    example = InputExample(\n",
    "        guid=0,\n",
    "        text_a=sent1,\n",
    "        text_b=sent2[\"true\"],\n",
    "        is_next=True,\n",
    "    )\n",
    "    tokenized_example = tokenize_example(example, tokenizer)\n",
    "    tokenized_examples = [tokenized_example]\n",
    "   \n",
    "    for sent2_alt in sent2[\"alt\"]:\n",
    "        example = InputExample(\n",
    "            guid=0,\n",
    "            text_a=sent1,\n",
    "            text_b=sent2_alt,\n",
    "            is_next=True,\n",
    "        )\n",
    "        tokenized_example = tokenize_example(example, tokenizer)\n",
    "        tokenized_examples.append(tokenized_example)\n",
    "        \n",
    "    if len(tokenized_examples) > 5:\n",
    "        tokenized_examples = [tokenized_examples[0]] + list(np.random.choice(tokenized_examples, 2))\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    features = [convert_example_to_features(ex, tokenizer, max_sequence_length, select_prob=0.0)\n",
    "                for ex in tokenized_examples]\n",
    "    \n",
    "    batch = features_to_data(features).to(device)\n",
    "\n",
    "    labels = torch.zeros(len(features), dtype=torch.long).to(device=device, )\n",
    "    if label:\n",
    "        labels[0] = 1\n",
    "    \n",
    "    loss = model(\n",
    "                batch.input_ids, \n",
    "                batch.segment_ids, \n",
    "                batch.input_mask, \n",
    "                labels,\n",
    "            )\n",
    "    \n",
    "    loss.backward()\n",
    "    if not steps % 5:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=71), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n",
      "tensor([[0.8163, 0.0467]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "for sent1, sent2 in tqdm.tqdm_notebook(zip(val_sent1s, val_sent2s), total=len(val_sent1s)):\n",
    "    example = InputExample(\n",
    "        guid=0,\n",
    "        text_a=sent1,\n",
    "        text_b=sent2[\"true\"],\n",
    "        is_next=True,\n",
    "    )\n",
    "    tokenized_example = tokenize_example(example, tokenizer)\n",
    "    features = [convert_example_to_features(tokenized_example, tokenizer, max_sequence_length, select_prob=0.0)]\n",
    "    batch = features_to_data(features).to(device)\n",
    "    with torch.no_grad():   \n",
    "        result = model(\n",
    "                batch.input_ids, \n",
    "                batch.segment_ids, \n",
    "                batch.input_mask, \n",
    "            )\n",
    "    print(result)\n",
    "    pred = int(torch.softmax(result, 1)[0, 1] > 0.3)\n",
    "    preds.append(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
